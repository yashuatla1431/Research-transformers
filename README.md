# ðŸ§  Research Transformers

This repository contains my implementations and notes as I reproduce and expand the **Transformer** and **GPT** architectures from scratch â€” inspired by Andrej Karpathy's *Zero to Hero* series.

---

### ðŸ“˜ Current Progress

- [x] Watched and followed Karpathy's GPT-from-scratch tutorial  
- [ ] Rebuilding GPT architecture from memory (Week 1 goal)  
- [ ] Implementing Encoder-Decoder Transformer  
- [ ] Pretraining GPT-mini on TinyStories dataset  
- [ ] Fine-tuning on instruction datasets (Alpaca, Dolly)

---

### ðŸ§© Learning Goal
To master LLM internals by building every component manually â€” tokenization, self-attention, pretraining, and instruction fine-tuning.

---

### ðŸ“… Week 1 Focus
- Rebuild Karpathy GPT from scratch
- Document architecture in notes
- Commit daily progress


